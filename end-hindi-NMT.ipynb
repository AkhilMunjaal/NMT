{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import pandas as pd\n",
        "import string\n",
        "from string import digits\n",
        "import re\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using positional encoding to re-inject order information\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim    # Dimension of embedding. 4 in the dummy example\n",
        "        self.dense_dim = dense_dim    # No. of neurons in dense layer\n",
        "        self.num_heads = num_heads    # No. of heads for MultiHead Attention layer\n",
        "        self.attention = layers.MultiHeadAttention(   # MultiHead Attention layer -\n",
        "            num_heads=num_heads, key_dim=embed_dim)   # see coloured pic above\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]    # encoders are stacked on top of the other.\n",
        "        )                                 # So output dimension is also embed_dim\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    # Call function based on figure above\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)  # Query: inputs, Value: inputs, Keys: Same as Values by default\n",
        "                                                  # Q: Can you see how this is self attention?\n",
        "        proj_input = self.layernorm_1(inputs + attention_output) # LayerNormalization; + Recall cat picture\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)  # LayerNormalization + Residual connection\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        # Define the layers. Let's point them out in the diagram\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        # Now we have 2 MultiHead Attention layers - one for ___ attention and one for ____ attention\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True #ensures that the layer will propagate its input mask to its outputs;\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1])) # sequence_length == input_shape[1]\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "              tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask_=None): # two inputs: decoder i/p and encoder o/p\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        attention_output_1 = self.attention_1(    # Q: What kind of attention?\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask) # Q: What will the causal_mask do?\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(  # Q: Is this self attention?\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,    # Key and Value coming from encoder\n",
        "            key=encoder_outputs\n",
        "        )\n",
        "\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " english (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " hindi (InputLayer)          [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " positional_embedding_2 (Po  (None, None, 256)            5125120   ['english[0][0]']             \n",
            " sitionalEmbedding)                                                                               \n",
            "                                                                                                  \n",
            " positional_embedding_3 (Po  (None, None, 256)            5125120   ['hindi[0][0]']               \n",
            " sitionalEmbedding)                                                                               \n",
            "                                                                                                  \n",
            " transformer_encoder_1 (Tra  (None, None, 256)            3155456   ['positional_embedding_2[0][0]\n",
            " nsformerEncoder)                                                   ']                            \n",
            "                                                                                                  \n",
            " transformer_decoder_1 (Tra  (None, None, 256)            5259520   ['positional_embedding_3[0][0]\n",
            " nsformerDecoder)                                                   ',                            \n",
            "                                                                     'transformer_encoder_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, None, 256)            0         ['transformer_decoder_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dense_9 (Dense)             (None, None, 20000)          5140000   ['dropout_1[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23805216 (90.81 MB)\n",
            "Trainable params: 23805216 (90.81 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "vocab_size = 20000\n",
        "sequence_length = 20\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"hindi\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs,mask_=None)\n",
        "\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "transformer.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0Y2NmDV7lF6"
      },
      "source": [
        "## Preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"Hindi_English_Truncated_Corpus.csv\",encoding='utf-8')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['english_sentence']=df['english_sentence'].apply(lambda x: x.lower())\n",
        "df['hindi_sentence']=df['hindi_sentence'].apply(lambda x: x.lower())\n",
        "\n",
        "exclude = set(string.punctuation) # Set of all special characters\n",
        "df['english_sentence']=df['english_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
        "df['hindi_sentence']=df['hindi_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
        "\n",
        "remove_digits = str.maketrans('', '', digits)\n",
        "\n",
        "df['english_sentence']=df['english_sentence'].apply(lambda x: x.translate(remove_digits))\n",
        "df['hindi_sentence']=df['hindi_sentence'].apply(lambda x: x.translate(remove_digits))\n",
        "\n",
        "df['hindi_sentence'] = df['hindi_sentence'].apply(lambda x: re.sub(\"[२३०८१५७९४६]\", \"\", x))\n",
        "\n",
        "# Remove extra hindices\n",
        "df['english_sentence']=df['english_sentence'].apply(lambda x: x.strip())\n",
        "df['hindi_sentence']=df['hindi_sentence'].apply(lambda x: x.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "df=df[df['source']=='ted']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nxxtnbv9D2-u",
        "outputId": "cced0084-5a8e-48d3-ede1-30b551a400f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('have microbial communities that are more similar', '[start] उनमे सूक्ष्म समुदायों में अधिक समानता है [end]')\n",
            "no. of pairs: 39881\n"
          ]
        }
      ],
      "source": [
        "# pre-processing. Separating input and output sequences\n",
        "\n",
        "text_pairs = []\n",
        "for _, row in df.iterrows():\n",
        "    english = row['english_sentence']\n",
        "    hindi = row['hindi_sentence']\n",
        "    hindi = \"[start] \" + hindi + \" [end]\"\n",
        "    text_pairs.append((english, hindi))\n",
        "\n",
        "import random\n",
        "print(random.choice(text_pairs))\n",
        "print(f\"no. of pairs: {len(text_pairs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "mcybUiQTD3Az"
      },
      "outputs": [],
      "source": [
        "#splitting data\n",
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hj9HjcGE7h4N",
        "outputId": "d9c888a3-e7b3-4a7f-f94d-898152820c9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ],
      "source": [
        "print(string.punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "kBc6EItWD3C3"
      },
      "outputs": [],
      "source": [
        "# Vectorizing the English and Hindi text pairs\n",
        "\n",
        "strip_chars = string.punctuation \n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "# Custom standardization function for hindi\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(    # Replace elements of input matching regex pattern with rewrite.\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_hindi_texts = [pair[1] for pair in train_pairs]\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_hindi_texts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "JMicSLvGD3E_"
      },
      "outputs": [],
      "source": [
        "\n",
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng, hindi):\n",
        "    # Q: What are eng and hindi pre and post re-assignment\n",
        "    eng = source_vectorization(eng)\n",
        "    hindi = target_vectorization(hindi)\n",
        "    return ({\n",
        "        \"english\": eng,           # encoder input\n",
        "        \"hindi\": hindi[:, :-1],    # decoder input Q: what is the first axis?\n",
        "    }, hindi[:, 1:])                  # decoder ouput\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, hindi_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    hindi_texts = list(hindi_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, hindi_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache() #Use in-memory caching to speed up preprocessing.\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4A2wVFs8Epoy",
        "outputId": "dcf30a36-ee29-4885-e3b5-d51448c86ea5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['hindi'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n",
            "tf.Tensor(\n",
            "[ 33  60 150  14 605   8  12   3   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0], shape=(20,), dtype=int64)\n",
            "tf.Tensor(\n",
            "[  80   19 1793   55  536    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0], shape=(20,), dtype=int64)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-03 21:28:36.458959: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "    print(f\"inputs['hindi'].shape: {inputs['hindi'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")\n",
        "    print(targets[3])\n",
        "    print(inputs['english'][3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54BdC0gg7pQS"
      },
      "source": [
        "## Traning and evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "_wd0SMFiZQJz",
        "outputId": "b1645a4f-03c2-4148-d469-768d8316e46f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "437/437 [==============================] - 397s 900ms/step - loss: 4.7944 - accuracy: 0.3090 - val_loss: 4.3784 - val_accuracy: 0.3361\n",
            "Epoch 2/4\n",
            "437/437 [==============================] - 394s 903ms/step - loss: 4.4277 - accuracy: 0.3494 - val_loss: 4.1788 - val_accuracy: 0.3617\n",
            "Epoch 3/4\n",
            "437/437 [==============================] - 397s 909ms/step - loss: 4.1511 - accuracy: 0.3798 - val_loss: 4.0810 - val_accuracy: 0.3734\n",
            "Epoch 4/4\n",
            "437/437 [==============================] - 391s 895ms/step - loss: 3.9129 - accuracy: 0.4051 - val_loss: 3.9952 - val_accuracy: 0.3863\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f77a4f1acd0>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]) # other metrics like Bleu....\n",
        "transformer.fit(train_ds, epochs=4, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaPdhwGbwqna",
        "outputId": "63ff0ef0-ddf7-41cf-a942-7ea10d58c81f",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-\n",
            "thats the easy thing\n",
            "[start] यह एक ही है [end]\n",
            "-\n",
            "you cant miss it when i tell you whats there\n",
            "[start] आप इसे नहीं कर सकते [end]\n",
            "-\n",
            "sneaking across the border\n",
            "[start] [UNK] के सभी [end]\n",
            "-\n",
            "they localized it into  different languages\n",
            "[start] वे इसे अलग अलग कर रहे हैं [end]\n"
          ]
        }
      ],
      "source": [
        "hindi_vocab = target_vectorization.get_vocabulary()\n",
        "hindi_index_lookup = dict(zip(range(len(hindi_vocab)), hindi_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization(\n",
        "            [decoded_sentence])[:, :-1]\n",
        "        predictions = transformer(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = hindi_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(4):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
